{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e8c740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "import damselfly as df\n",
    "\n",
    "PATH = '/storage/home/adz6/group/project'\n",
    "RESULTPATH = os.path.join(PATH, 'results/damselfly')\n",
    "PLOTPATH = os.path.join(PATH, 'plots/damselfly')\n",
    "DATAPATH = os.path.join(PATH, 'damselfly/data/datasets')\n",
    "\n",
    "\"\"\"\n",
    "Date: 7/20/2021\n",
    "Description: prototype autoencoder\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccf1852",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvDecoder2(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, deconv_block1, deconv_block2, maxunpool_kerns, linear_block):\n",
    "        super(ConvDecoder2, self).__init__()\n",
    "        \n",
    "        self.layer1 = DecLinearBlock(linear_block)\n",
    "        \n",
    "        self.unflatten = torch.nn.Unflatten(dim=1, unflattened_size = (deconv_block1[0][0], linear_block[0][0] ))\n",
    "        self.maxunpool1 = torch.nn.MaxUnpool1d(kernel_size=maxunpool_kerns[0])\n",
    "        self.layer2 = DeconvBlock(deconv_block1)\n",
    "        self.maxunpool2 = torch.nn.MaxUnpool1d(kernel_size=maxunpool_kerns[1])\n",
    "        self.layer3 = DeconvBlock(deconv_block2)\n",
    "        \n",
    "    def forward(self, x, indices):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        \n",
    "        x = self.unflatten(x)\n",
    "        x = self.maxunpool1(x, indices[-1])\n",
    "        x = self.layer2(x)\n",
    "        x = self.maxunpool2(x, indices[-2])\n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class ConvEncoder2(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_block1, conv_block2, maxpool_kerns, linear_block ):\n",
    "        super(ConvEncoder2, self).__init__()\n",
    "        \n",
    "        self.layer1 = ConvBlock(conv_block1)\n",
    "        self.maxpool1 = torch.nn.MaxPool1d(kernel_size=maxpool_kerns[0], return_indices=True)\n",
    "        self.layer2 = ConvBlock(conv_block2)\n",
    "        self.maxpool2 = torch.nn.MaxPool1d(kernel_size=maxpool_kerns[1], return_indices=True)\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        \n",
    "        self.layer3 = EncLinearBlock(linear_block)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x, indices1 = self.maxpool1(x)\n",
    "        x = self.layer2(x)\n",
    "        x, indices2 = self.maxpool2(x)\n",
    "        x = self.flatten(x)\n",
    "        \n",
    "        x = self.layer3(x)\n",
    "        \n",
    "        return x, (indices1, indices2)\n",
    "    \n",
    "class ConvBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_list):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        \n",
    "        self.activation = torch.nn.ReLU\n",
    "        self.layer1 = self._make_block(conv_list, self.activation)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.layer1(x)\n",
    "        \n",
    "    def _make_block(self, conv_list, activation):\n",
    "        layers = []\n",
    "        \n",
    "        for item in conv_list:\n",
    "            layers.append(torch.nn.Conv1d(item[0], item[1], kernel_size=item[2], \n",
    "                                          dilation=item[3], stride=1, padding=item[2]//2, \n",
    "                                          padding_mode='circular'))\n",
    "            layers.append(activation())\n",
    "            \n",
    "        return torch.nn.Sequential(*layers)\n",
    "    \n",
    "class DeconvBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, deconv_list):\n",
    "        super(DeconvBlock, self).__init__()\n",
    "        \n",
    "        self.activation = torch.nn.ReLU\n",
    "        self.layer1 = self._make_block(deconv_list, self.activation)\n",
    "        \n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.layer1(x)\n",
    "        \n",
    "    def _make_block(self, deconv_list, activation):\n",
    "        layers = []\n",
    "        \n",
    "        for item in deconv_list:\n",
    "            layers.append(activation())\n",
    "            layers.append(torch.nn.ConvTranspose1d(item[0], item[1], kernel_size=item[2], \n",
    "                                                   stride=1, padding=item[2]//2))\n",
    "            \n",
    "            \n",
    "        return torch.nn.Sequential(*layers)\n",
    "    \n",
    "class EncLinearBlock(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, linear_block):\n",
    "        super(EncLinearBlock, self).__init__()\n",
    "        \n",
    "        self.activation = torch.nn.ReLU\n",
    "        self.layer1 = self._make_block(linear_block, self.activation)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.layer1(x)\n",
    "    \n",
    "    def _make_block(self, linear_block, activation):\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(linear_block)):\n",
    "            layers.append(torch.nn.Linear(linear_block[i][0], linear_block[i][1]))\n",
    "            if i < len(linear_block) - 1:\n",
    "                layers.append(activation())\n",
    "\n",
    "        return torch.nn.Sequential(*layers)\n",
    "\n",
    "class DecLinearBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, linear_block):\n",
    "        super(DecLinearBlock, self).__init__()\n",
    "        \n",
    "        self.activation = torch.nn.ReLU\n",
    "        self.layer1 = self._make_block(linear_block, self.activation)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        return self.layer1(x)\n",
    "    \n",
    "    def _make_block(self, linear_block, activation):\n",
    "        layers = []\n",
    "        \n",
    "        for i in range(len(linear_block)):\n",
    "            layers.append(activation())\n",
    "            layers.append(torch.nn.Linear(linear_block[i][0], linear_block[i][1]))\n",
    "\n",
    "        return torch.nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46947424",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAE(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 conv_block1, \n",
    "                 conv_block2, \n",
    "                 maxpool_kerns, \n",
    "                 enc_linear_block, \n",
    "                 deconv_block1, \n",
    "                 deconv_block2, \n",
    "                 maxunpool_kerns, \n",
    "                 dec_linear_block\n",
    "                ):\n",
    "        super(ConvAE, self).__init__()\n",
    "        \n",
    "        self.encoder = ConvEncoder2(conv_block1, conv_block2, maxpool_kerns, enc_linear_block)\n",
    "        self.decoder = ConvDecoder2(deconv_block1, deconv_block2, maxunpool_kerns, dec_linear_block)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x, indices = self.encoder(x)\n",
    "        x = self.decoder(x, indices)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0b5d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "convblock_list1 = [(3, 20, 13, 1), (20, 20, 13, 1)]\n",
    "convblock_list2 = [(20, 40, 7, 1), (40, 40, 7, 1)]\n",
    "\n",
    "deconvblock_list1 = [(40, 40, 7, 1), (40, 20, 7, 1)]\n",
    "deconvblock_list2 = [(20, 20, 13, 1), (20, 3, 13, 1)]\n",
    "\n",
    "maxpool_kerns = [8, 4]\n",
    "maxunpool_kerns = [4, 8]\n",
    "\n",
    "enc_linear_block = [(40 * 8192 // (8 * 4), 512 ), (512, 256)]\n",
    "dec_linear_block = [(256, 512), (512, 40 * 8192 // (8 * 4))]\n",
    "\n",
    "encoder_list = [convblock_list1, convblock_list2, maxpool_kerns, enc_linear_block]\n",
    "decoder_list = [deconvblock_list1, deconvblock_list2, maxunpool_kerns, dec_linear_block]\n",
    "\n",
    "enc = ConvEncoder2(convblock_list1, convblock_list2, maxpool_kerns, enc_linear_block)\n",
    "block = ConvBlock(convblock_list1)\n",
    "dec = ConvDecoder2(deconvblock_list1, deconvblock_list2, maxunpool_kerns, dec_linear_block)\n",
    "\n",
    "ae = ConvAE(*encoder_list, *decoder_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e895a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc2dfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((10, 3, 8192))\n",
    "y = enc.forward(x)[0]\n",
    "enc_indices = enc.forward(x)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d9c2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = dec.forward(y, enc_indices)\n",
    "print(z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba5d181",
   "metadata": {},
   "outputs": [],
   "source": [
    "ae.forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9095e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ConvRelu(in_f, out_f, kernel, dilation):\n",
    "\n",
    "    return torch.nn.Sequential(\n",
    "                    torch.nn.Conv1d(in_f, out_f, kernel_size=kernel, stride=1, dilation=dilation),\n",
    "                    torch.nn.ReLU()\n",
    "                         )\n",
    "def ConvTransRelu(in_f, out_f, kernel, dilation):\n",
    "\n",
    "    return torch.nn.Sequential(\n",
    "                    torch.nn.ReLU(),\n",
    "                    torch.nn.ConvTranspose1d(in_f, out_f, kernel_size=kernel, stride=1, dilation=dilation)\n",
    "                    \n",
    "                         )\n",
    "\n",
    "def ConvMaxpoolBlock(conv_in_f, conv_out_f, conv_kernel, conv_dilation, maxpool_kernel):\n",
    "\n",
    "    conv_relu_blocks = []\n",
    "    for convset in zip(conv_in_f, conv_out_f, conv_kernel, conv_dilation):\n",
    "        #print(convset)\n",
    "        conv_relu_blocks.append(ConvRelu(convset[0], convset[1], convset[2], convset[3]))\n",
    "    #[ConvRelu(convset[0], convset[1], convset[2], convset[3]) for convset in zip(conv_in_f, conv_out_f, conv_kernel, conv_dilation)]\n",
    "    \n",
    "    return torch.nn.Sequential(\n",
    "                        *conv_relu_blocks,\n",
    "                        torch.nn.MaxPool1d(kernel_size=maxpool_kernel)\n",
    "                        )\n",
    "\n",
    "def ConvMaxUnpoolBlock(conv_in_f, conv_out_f, conv_kernel, conv_dilation, maxunpool_kernel):\n",
    "\n",
    "    conv_trans_relu_blocks = []\n",
    "    for convset in zip(conv_in_f, conv_out_f, conv_kernel, conv_dilation):\n",
    "        #print(convset)\n",
    "        conv_trans_relu_blocks.append(ConvTransRelu(convset[0], convset[1], convset[2], convset[3]))\n",
    "    #[ConvRelu(convset[0], convset[1], convset[2], convset[3]) for convset in zip(conv_in_f, conv_out_f, conv_kernel, conv_dilation)]\n",
    "    \n",
    "    return torch.nn.Sequential(\n",
    "                        torch.nn.MaxUnpool1d(kernel_size=maxunpool_kernel),\n",
    "                        *conv_trans_relu_blocks\n",
    "                        )\n",
    "\n",
    "def StackConvMaxpool(conv_max_list):\n",
    "    conv_max_blocks = []\n",
    "    for conv_max_set in conv_max_list:\n",
    "        #print(conv_max_set)\n",
    "        conv_max_blocks.append(ConvMaxpoolBlock(conv_max_set[0], conv_max_set[1], conv_max_set[2], conv_max_set[3], conv_max_set[4]))\n",
    "    #[ConvMaxpoolBlock(conv_max_set[0], conv_max_set[1], conv_max_set[2], conv_max_set[3], conv_max_set[4]) for conv_max_set in conv_max_list]\n",
    "    \n",
    "    return torch.nn.Sequential(*conv_max_blocks)\n",
    "\n",
    "def StackConvMaxUnpool(conv_unmax_list):\n",
    "    conv_unmax_blocks = []\n",
    "    for conv_unmax_set in conv_unmax_list:\n",
    "        #print(conv_max_set)\n",
    "        conv_unmax_blocks.append(ConvMaxUnpoolBlock(conv_unmax_set[0], conv_unmax_set[1], conv_unmax_set[2], conv_unmax_set[3], conv_unmax_set[4]))\n",
    "    #[ConvMaxpoolBlock(conv_max_set[0], conv_max_set[1], conv_max_set[2], conv_max_set[3], conv_max_set[4]) for conv_max_set in conv_max_list]\n",
    "    \n",
    "    return torch.nn.Sequential(*conv_unmax_blocks)\n",
    "\n",
    "def LinearRelu(in_f, out_f):\n",
    "\n",
    "    return torch.nn.Sequential(\n",
    "                        torch.nn.Linear(in_f, out_f),\n",
    "                        torch.nn.ReLU(),\n",
    "                        )\n",
    "\n",
    "def EncLinear(in_f, out_f):\n",
    "\n",
    "    #linear_blocks = [LinearRelu(linearset[0], linearset[1]) for linearset in zip(in_f, out_f)]\n",
    "    layers = []\n",
    "    for i in range(len(in_f)):\n",
    "        layers.append(torch.nn.Linear(in_f[i], out_f[i]))\n",
    "        if i < len(in_f) - 1:\n",
    "            layers.append(torch.nn.ReLU())\n",
    "    \n",
    "    return torch.nn.Sequential(*layers)\n",
    "\n",
    "def DecLinear(in_f, out_f):\n",
    "\n",
    "    #linear_blocks = [LinearRelu(linearset[0], linearset[1]) for linearset in zip(in_f, out_f)]\n",
    "    layers = []\n",
    "    for i in range(len(in_f)):\n",
    "        layers.append(torch.nn.ReLU())\n",
    "        layers.append(torch.nn.Linear(in_f[i], out_f[i]))\n",
    "        \n",
    "    \n",
    "    return torch.nn.Sequential(*layers)\n",
    "\n",
    "\n",
    "class ConvEncoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_list, linear_list):\n",
    "        super(ConvEncoder, self).__init__()\n",
    "        \n",
    "        self.conv = StackConvMaxpool(conv_list)\n",
    "        self.flatten = torch.nn.Flatten(start_dim=1)\n",
    "        self.linear = EncLinear(linear_list[0], linear_list[1])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class ConvDecoder(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_list, linear_list):\n",
    "        super(ConvDecoder, self).__init__()\n",
    "        \n",
    "        self.linear = DecLinear(linear_list[0], linear_list[1])\n",
    "        self.unflatten = torch.nn.Unflatten(dim=1, unflattened_size = (conv_list[0][0][0], linear_list[-1][-1] // (conv_list[0][0][0])))\n",
    "        self.deconv = StackConvMaxUnpool(conv_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.deconv(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        \n",
    "def CalcConvMaxpoolOutputSize(conv_max_list, ninput_ch, ninput):\n",
    "    conv_stack = StackConvMaxpool(conv_max_list)\n",
    "    \n",
    "    x = torch.rand((1, ninput_ch, ninput))\n",
    "    #print(x.shape)\n",
    "    x = conv_stack(x)\n",
    "    \n",
    "    size = x.size()[1:]\n",
    "    num_features = 1\n",
    "    for s in size:\n",
    "        num_features *= s\n",
    "    \n",
    "    x = x.view(-1, num_features)\n",
    "    \n",
    "    return int(x.shape[-1])\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba9506c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51fd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_conv_list = [\n",
    "                [\n",
    "                    [3, 20], # in_f \n",
    "                    [20, 20], # out_f\n",
    "                    [12, 12], # conv_kernels\n",
    "                    [1, 1], # dilations\n",
    "                    12 # maxpool_kernel + size\n",
    "                ],\n",
    "                [\n",
    "                    [20, 40],\n",
    "                    [40, 40],\n",
    "                    [6, 6],\n",
    "                    [1, 1],\n",
    "                    6\n",
    "                ],\n",
    "                [\n",
    "                    [40, 80],\n",
    "                    [80, 80],\n",
    "                    [3, 3],\n",
    "                    [1, 1],\n",
    "                    3\n",
    "                ]\n",
    "            ]\n",
    "\n",
    "enc_linear_list = [\n",
    "                [df.models.CalcConvMaxpoolOutputSize(conv_list, 3, 8192), 416],\n",
    "                [416, 213],\n",
    "            ]\n",
    "\n",
    "dec_conv_list = [\n",
    "                [\n",
    "                    [80,80],\n",
    "                    [80, 40],\n",
    "                    [3, 3],\n",
    "                    [1, 1],\n",
    "                    3\n",
    "                ],\n",
    "                [\n",
    "                    [40, 40],\n",
    "                    [40, 20],\n",
    "                    [6, 6],\n",
    "                    [1, 1],\n",
    "                    6\n",
    "                ],\n",
    "                [\n",
    "                    [20, 20], # in_f \n",
    "                    [20, 3], # out_f\n",
    "                    [12, 12], # conv_kernels\n",
    "                    [1, 1], # dilations\n",
    "                    12 # maxpool_kernel + size\n",
    "                ],\n",
    "                \n",
    "                \n",
    "            ]\n",
    "\n",
    "dec_linear_list = [\n",
    "                [213, 416],\n",
    "                [416, df.models.CalcConvMaxpoolOutputSize(conv_list, 3, 8192)]\n",
    "            ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834b3c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ConvEncoder(enc_conv_list, enc_linear_list)\n",
    "decoder = ConvDecoder(dec_conv_list, dec_linear_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b62f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(12, 3, 8192)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc25135f",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.forward(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3b35af",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487338d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06705189",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = encoder.forward(x)\n",
    "print(decoder.forward(y).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14be80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm1d(planes):\n",
    "    \n",
    "    return torch.nn.BatchNorm1d(planes)\n",
    "\n",
    "def conv1xn(in_planes, out_planes, kernel_size, stride = 1):\n",
    "\n",
    "    return torch.nn.Conv1d(in_planes, out_planes, kernel_size=kernel_size, stride=stride, dilation=1, padding_mode='circular', padding = kernel_size // 2, bias=False)\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \n",
    "    return torch.nn.Conv1d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "def downsample(in_planes, out_planes, stride = 1):\n",
    "    \n",
    "    return torch.nn.Sequential(conv1x1(in_planes, out_planes, stride = stride), norm1d(out_planes))\n",
    "\n",
    "class ResBlock(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, inplanes, planes, in_kernel, out_kernel, stride=1):\n",
    "        \n",
    "        super(ResBlock, self).__init__()\n",
    "        \n",
    "        self.inplanes = inplanes\n",
    "        \n",
    "        self.planes = planes\n",
    "        \n",
    "        self.conv1 = conv1xn(self.inplanes, self.planes, in_kernel, stride=stride)\n",
    "        \n",
    "        self.bn1 = norm1d(self.planes)\n",
    "        \n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.conv2 = conv1xn(self.planes, self.planes, out_kernel)\n",
    "        \n",
    "        self.bn2 = norm1d(self.planes)\n",
    "        \n",
    "        if self.inplanes != self.planes:\n",
    "        \n",
    "            self.downsample = downsample(self.inplanes, self.planes, stride=stride)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        identity = x\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        #print(out.shape, identity.shape)\n",
    "        \n",
    "        if self.inplanes != self.planes:\n",
    "        \n",
    "            identity = self.downsample(x)\n",
    "            \n",
    "        #print(out.shape, identity.shape)\n",
    "        \n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        return out\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e943d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class resnet1d(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, block, block_list, stride=4, nclass = 2):\n",
    "        \n",
    "        super(resnet1d, self).__init__()\n",
    "        \n",
    "        self.inplanes = 64\n",
    "        self.kernel_size = 7\n",
    "        self.stride = stride\n",
    "        self.output_size = 4096 // 4 ** 3\n",
    "        \n",
    "        self.conv1 = conv1xn(2, self.inplanes, 7, stride=1)\n",
    "        self.bn1 = norm1d(self.inplanes)\n",
    "        self.relu = torch.nn.ReLU(inplace=True)\n",
    "        self.maxpool = torch.nn.MaxPool1d(2, padding = 0)\n",
    "        \n",
    "        self.layer1 = self._make_layer(block, self.inplanes, self.inplanes, self.kernel_size, block_list[0], self.stride)\n",
    "        self.layer2 = self._make_layer(block, self.inplanes, 2 * self.inplanes, self.kernel_size, block_list[1], self.stride)\n",
    "        self.layer3 = self._make_layer(block, 2 * self.inplanes, 4 * self.inplanes, self.kernel_size, block_list[2], self.stride)\n",
    "        self.layer4 = self._make_layer(block, 4 * self.inplanes, 8 * self.inplanes, self.kernel_size, block_list[3], self.stride)\n",
    "        \n",
    "        self.avgpool = torch.nn.AdaptiveAvgPool1d(1)\n",
    "        \n",
    "        self.fc = torch.nn.Linear(8 * self.inplanes, nclass)\n",
    "        \n",
    "    def _make_layer(self, block, inplanes, outplanes, kernel_size, blocks, stride):\n",
    "        \n",
    "        layers = []\n",
    "        layer_planes = inplanes\n",
    "        \n",
    "        if layer_planes == outplanes:\n",
    "            layers.append(block(layer_planes, outplanes, kernel_size, kernel_size))\n",
    "        else:\n",
    "            layers.append(block(layer_planes, outplanes, kernel_size, kernel_size, stride=stride))\n",
    "        \n",
    "        layer_planes = outplanes\n",
    "            \n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(layer_planes, layer_planes, kernel_size, kernel_size))\n",
    "            \n",
    "        return torch.nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4078ebb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = resnet1d(ResBlock, [2,2,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336cf15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((10, 2, 8192))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fccf0d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a2c4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool = torch.nn.MaxPool1d(2, padding = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ca3b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "maxpool(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d1af10",
   "metadata": {},
   "outputs": [],
   "source": [
    "4096 // 4 ** 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5f36f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
