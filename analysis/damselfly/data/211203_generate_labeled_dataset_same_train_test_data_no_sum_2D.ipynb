{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c4d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import torch\n",
    "import h5py\n",
    "import os\n",
    "import sys\n",
    "import scipy\n",
    "import damselfly as df\n",
    "import mayfly as mf\n",
    "import scipy.signal\n",
    "import scipy.stats\n",
    "import scipy.interpolate\n",
    "\n",
    "PATH = '/storage/home/adz6/group/project'\n",
    "RESULTPATH = os.path.join(PATH, 'results/damselfly')\n",
    "PLOTPATH = os.path.join(PATH, 'plots/damselfly')\n",
    "DATAPATH = os.path.join(PATH, 'datasets/data')\n",
    "#SIMDATAPATH = os.path.join(PATH, 'damselfly/data/sim_data')\n",
    "\n",
    "\"\"\"\n",
    "Date: 6/25/2021\n",
    "Description: template\n",
    "\"\"\"\n",
    "\n",
    "def GetFreqLabel(signal_metadata, index, kass_data, kass_metadata):\n",
    "    \n",
    "    #print(signal_metadata.iloc[index]['theta_min'])\n",
    "    \n",
    "    # need to check Kass index since the beamforming process mixes up the indices\n",
    "    kass_index = kass_metadata[(kass_metadata['energy'] == signal_metadata.iloc[index]['energy']) & \n",
    "                        (kass_metadata['theta_min'] == signal_metadata.iloc[index]['theta_min']) & \n",
    "                       (kass_metadata['x_min'] == signal_metadata.iloc[index]['x_min'])\n",
    "                       ].index[0]\n",
    "    \n",
    "    # takes the starting cyclotron frequncy as the label\n",
    "    frequency = kass_data['fc'][kass_index, 0]\n",
    "    \n",
    "    return frequency\n",
    "\n",
    "def CreateGroups(h5file, config):\n",
    "    for i, grp in enumerate(['train', 'val', 'test']):\n",
    "        h5file.create_group(grp)\n",
    "        h5file[grp].create_group('meta')\n",
    "        if i == 0:\n",
    "            h5file[grp].create_dataset('data', config['train_shape'], dtype='f4')\n",
    "            h5file[grp].create_dataset('label', (config['train_shape'][0],), dtype='f4')\n",
    "            \n",
    "            for j, key in enumerate(['energy', 'x_min', 'theta_min']):\n",
    "                h5file[grp]['meta'].create_dataset(key, (config['train_shape'][0],), dtype='f4')\n",
    "        else:\n",
    "            h5file[grp].create_dataset('data', config['test_shape'], dtype='f4')\n",
    "            h5file[grp].create_dataset('label', (config['test_shape'][0],), dtype='f4')\n",
    "            \n",
    "            for j, key in enumerate(['energy', 'x_min', 'theta_min']):\n",
    "                h5file[grp]['meta'].create_dataset(key, (config['test_shape'][0],), dtype='f4')\n",
    "        \n",
    "\n",
    "def GetEnergyLabel(signal_metadata, index):\n",
    "    \n",
    "    # takes the energy from the metadata\n",
    "    energy = signal_metadata.iloc[index]['energy']\n",
    "    \n",
    "    return energy\n",
    "\n",
    "def DataSlicer(data, islice, inds, nch, slicesize):\n",
    "    \n",
    "    print(data[inds, :].reshape((inds.size, nch, data.shape[-1] // nch))[:, :, islice * slicesize:(islice + 1) * slicesize].shape)\n",
    "    \n",
    "    return data[inds, :].reshape((inds.size, nch, data.shape[-1] // nch))[:, :, islice * slicesize:(islice + 1) * slicesize]\n",
    "\n",
    "#def AddNoise(data, var):\n",
    "#    rng = np.random.default_rng()\n",
    "    \n",
    "#    noise = rng.multivariate_normal([0,0], np.eye(2) * var / 2, data.size)\n",
    "    \n",
    "#    noise = noise[:, 0] + 1j * noise[:, 1]\n",
    "    \n",
    "#    return data + noise.reshape(data.shape)\n",
    "\n",
    "# same signals in each dataset. Noise to be added during training to save storage space.\n",
    "\n",
    "def CreateDLDataset(config, data, metadata):\n",
    "    \n",
    "    h5file = h5py.File(name, 'w')\n",
    "    \n",
    "    CreateGroups(h5file, config)\n",
    "    \n",
    "    labels = np.zeros(data.data.shape[0])\n",
    "    \n",
    "    chunk_inds = np.array_split(np.arange(0, data.data.shape[0], 1), config['nchunk'])\n",
    "    \n",
    "    #if config['label'] == 'freq':\n",
    "    #    for i in range(data.data.shape[0]):\n",
    "            #labels[i] = GetFreqLabel(metadata, i, kass_data, kass_metadata)\n",
    "    if config['label'] == 'energy':\n",
    "        for i in range(data.data.shape[0]):\n",
    "            labels[i] = GetEnergyLabel(metadata, i)        \n",
    "    if config['label'] == 'class':\n",
    "        labels = np.ones(data.data.shape[0])\n",
    "        \n",
    "    for i, grp in enumerate(['train', 'val', 'test']):\n",
    "        print(f'Starting {grp}')\n",
    "        for islice in range(config['nslice']):\n",
    "            \n",
    "            for ichunk in range(config['nchunk']):\n",
    "\n",
    "                data_slice = DataSlicer(data.data, islice, chunk_inds[ichunk], config['nch'], config['slicesize'])\n",
    "\n",
    "                data_fft = np.fft.fftshift(np.fft.fft(data_slice, axis=-1), axes=(-1)) / config['slicesize']\n",
    "\n",
    "                h5file[grp]['data'][chunk_inds[ichunk], 2 * islice, :, :] = data_fft.real\n",
    "                h5file[grp]['data'][chunk_inds[ichunk], 2 * islice + 1, :, :] = data_fft.imag\n",
    "                #h5file[grp]['data'][:, 3 * islice + 2, :] = abs(data_noise) ** 2\n",
    "            \n",
    "        h5file[grp]['label'][:] = labels\n",
    "\n",
    "        h5file[grp]['meta']['energy'][:] = np.array(metadata['energy'].array)\n",
    "        h5file[grp]['meta']['x_min'][:] = np.array(metadata['x_min'].array)\n",
    "        h5file[grp]['meta']['theta_min'][:] = np.array(metadata['theta_min'].array)\n",
    "        print(f'Done with {grp}')\n",
    "    h5file.close()\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f17fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(DATAPATH,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(os.path.join(PATH, 'datasets', 'kass'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23fdf81",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28868572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# signal data\n",
    "data = mf.data.MFDataset(os.path.join(DATAPATH, '211027_84_25_2cm.h5'))\n",
    "metadata = pd.DataFrame(data.metadata)\n",
    "\n",
    "# kass data\n",
    "# h5kass_data = h5py.File(os.path.join(PATH, 'datasets', 'kass', '211129_sens_est_dense_grid_84.5_0cm_kass.h5'), 'r')\n",
    "\n",
    "#kass_data = h5kass_data['kass']\n",
    "#kass_metadata = {}\n",
    "#for i, key in enumerate(h5kass_data['meta'].keys()):\n",
    "#    kass_metadata[key] = h5kass_data['meta'][key][:]\n",
    "    \n",
    "#kass_metadata = pd.DataFrame.from_dict(kass_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f2f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb50d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "1474560 // 60\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ad80cc",
   "metadata": {},
   "source": [
    "# define output dataset parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5244790",
   "metadata": {},
   "outputs": [],
   "source": [
    "nsignal = data.shape[0]\n",
    "nsample = data.shape[1]\n",
    "\n",
    "nchunk = 4\n",
    "\n",
    "# same signals in train, test, val sets. Different noise samples added to signals at run time\n",
    "#ncopies_train = 10\n",
    "#ncopies_test = 4\n",
    "\n",
    "nslice = 1\n",
    "ninput_ch = 2 # real, imag\n",
    "slicesize = 2 * 8192\n",
    "nantenna = 60\n",
    "\n",
    "train_shape = (nsignal, nslice * ninput_ch, nantenna, slicesize)\n",
    "test_shape = (nsignal, nslice * ninput_ch, nantenna, slicesize)\n",
    "\n",
    "noise_temp = 10\n",
    "fsample = 200e6\n",
    "system_z = 50\n",
    "nch = 60\n",
    "kB = 1.38e-23\n",
    "\n",
    "noise_var = kB * nch * noise_temp * system_z * fsample\n",
    "noise_var_per_bin = noise_var / slicesize\n",
    "\n",
    "name = os.path.join(DATAPATH, 'dl', '211203_dl_classification_84_25_2cm_slice1_sample2x8192_no_sum.h5')\n",
    "label = 'class'\n",
    "\n",
    "config = {\n",
    "    'train_shape': train_shape,\n",
    "    'test_shape': test_shape,\n",
    "    'nsignal': nsignal,\n",
    "    'nsample': nsample,\n",
    "    'nslice': nslice,\n",
    "    'ninput_ch': ninput_ch,\n",
    "    'slicesize': slicesize,\n",
    "    'noise_temp': noise_temp,\n",
    "    'fsample': fsample,\n",
    "    'system_z': system_z,\n",
    "    'nch': nch,\n",
    "    'noise_var': noise_var,\n",
    "    'noise_var_per_bin': noise_var_per_bin,\n",
    "    'name': name,\n",
    "    'label': label,\n",
    "    'nchunk': nchunk,\n",
    "}\n",
    "\n",
    "\n",
    "CreateDLDataset(config, data, metadata)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e744627",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
