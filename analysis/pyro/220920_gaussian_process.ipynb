{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8933a22d",
   "metadata": {},
   "source": [
    "# Gaussian Processess\n",
    "\n",
    "Intuitive way to define priors over functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff7f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "import pyro\n",
    "import pyro.contrib.gp as gp\n",
    "import pyro.distributions as dist\n",
    "\n",
    "from matplotlib.animation import FuncAnimation\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "#smoke_test = \"CI\" in os.environ\n",
    "#assert pyro.__version__.startswith('1.8.2')\n",
    "pyro.set_rng_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "070324de",
   "metadata": {},
   "source": [
    "Helper plotting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e633f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that this helper function does three different things:\n",
    "# (i) plots the observed data;\n",
    "# (ii) plots the predictions from the learned GP after conditioning on data;\n",
    "# (iii) plots samples from the GP prior (with no conditioning on observed data)\n",
    "\n",
    "\n",
    "def plot(\n",
    "    plot_observed_data=False,\n",
    "    plot_predictions=False,\n",
    "    n_prior_samples=0,\n",
    "    model=None,\n",
    "    kernel=None,\n",
    "    n_test=500,\n",
    "    ax=None,\n",
    "):\n",
    "\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    if plot_observed_data:\n",
    "        ax.plot(X.numpy(), y.numpy(), \"kx\")\n",
    "    if plot_predictions:\n",
    "        Xtest = torch.linspace(-0.5, 5.5, n_test)  # test inputs\n",
    "        # compute predictive mean and variance\n",
    "        with torch.no_grad():\n",
    "            if type(model) == gp.models.VariationalSparseGP:\n",
    "                mean, cov = model(Xtest, full_cov=True)\n",
    "            else:\n",
    "                mean, cov = model(Xtest, full_cov=True, noiseless=False)\n",
    "        sd = cov.diag().sqrt()  # standard deviation at each input point x\n",
    "        ax.plot(Xtest.numpy(), mean.numpy(), \"r\", lw=2)  # plot the mean\n",
    "        ax.fill_between(\n",
    "            Xtest.numpy(),  # plot the two-sigma uncertainty about the mean\n",
    "            (mean - 2.0 * sd).numpy(),\n",
    "            (mean + 2.0 * sd).numpy(),\n",
    "            color=\"C0\",\n",
    "            alpha=0.3,\n",
    "        )\n",
    "    if n_prior_samples > 0:  # plot samples from the GP prior\n",
    "        Xtest = torch.linspace(-0.5, 5.5, n_test)  # test inputs\n",
    "        noise = (\n",
    "            model.noise\n",
    "            if type(model) != gp.models.VariationalSparseGP\n",
    "            else model.likelihood.variance\n",
    "        )\n",
    "        cov = kernel.forward(Xtest) + noise.expand(n_test).diag()\n",
    "        samples = dist.MultivariateNormal(\n",
    "            torch.zeros(n_test), covariance_matrix=cov\n",
    "        ).sample(sample_shape=(n_prior_samples,))\n",
    "        ax.plot(Xtest.numpy(), samples.numpy().T, lw=2, alpha=0.4)\n",
    "\n",
    "    ax.set_xlim(-0.5, 5.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621ecb05",
   "metadata": {},
   "source": [
    "Data are 20 points sampled from our function y(x).\n",
    "x is randomly sampled on a uniform interval.\n",
    "y has gaussian noise added.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fbb202",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 20\n",
    "X = dist.Uniform(0.0, 5.0).sample(sample_shape=(N,))\n",
    "y = 0.5 * torch.sin(3 * X) + dist.Normal(0.0, 0.2).sample(sample_shape=(N,))\n",
    "\n",
    "plot(plot_observed_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825167c",
   "metadata": {},
   "source": [
    "define our kernel. Use the RBF kernel. The kernel defines the gaussian process which is a function over functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75664f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = gp.kernels.RBF(\n",
    "    input_dim=1, variance=torch.tensor(6.0), lengthscale=torch.tensor(0.05)\n",
    ")\n",
    "gpr = gp.models.GPRegression(X,y, kernel, noise=torch.tensor(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf459f0",
   "metadata": {},
   "source": [
    "plot some samples from the prior which is our unconditioned GP regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c4fced",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(model=gpr, kernel=kernel, n_prior_samples=2)\n",
    "_ = plt.ylim((-8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6b02cf",
   "metadata": {},
   "source": [
    "do inference to learn the kernel hyperparameters from the data. our loss function needs a model and a guide (SVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18d3ee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(gpr.parameters(), lr=0.005)\n",
    "loss_fn = pyro.infer.Trace_ELBO().differentiable_loss\n",
    "losses = []\n",
    "variances = []\n",
    "lengthscales = []\n",
    "noises = []\n",
    "num_steps = 2000 #if not smoke_test else 2\n",
    "for i in range(num_steps):\n",
    "    variances.append(gpr.kernel.variance.item())\n",
    "    noises.append(gpr.noise.item())\n",
    "    lengthscales.append(gpr.kernel.lengthscale.item())\n",
    "    optimizer.zero_grad()\n",
    "    loss = loss_fn(gpr.model, gpr.guide)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aff023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's plot the loss curve after 2000 steps of training\n",
    "def plot_loss(loss):\n",
    "    plt.plot(loss)\n",
    "    plt.xlabel(\"Iterations\")\n",
    "    _ = plt.ylabel(\"Loss\")  # supress output text\n",
    "\n",
    "\n",
    "plot_loss(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b7c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(model=gpr, plot_observed_data=True, plot_predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf9e3e2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
